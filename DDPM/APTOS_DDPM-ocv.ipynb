{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as TF\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "import torch\n",
    "from torch.cuda import memory_allocated\n",
    "from torch.cuda import empty_cache\n",
    "from torch.cuda import memory_cached\n",
    "from torch.cuda import reset_max_memory_allocated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
    "        super().__init__()\n",
    "\n",
    "        half_dim = time_emb_dims // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "\n",
    "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
    "\n",
    "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\n",
    "        self.time_blocks = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
    "        )\n",
    "\n",
    "    def forward(self, time):\n",
    "        return self.time_blocks(time)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)  # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n",
    "        h, _ = self.mhsa(h, h, h)  # [B, H*W, C]\n",
    "        h = h.swapaxes(2, 1).view(B, self.channels, H, W)  # [B, C, H*W] --> [B, C, H, W]\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels, dropout_rate=0.1, time_emb_dims=512, apply_attention=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.act_fn = nn.SiLU()\n",
    "        # Group 1\n",
    "        self.normlize_1 = nn.GroupNorm(num_groups=8, num_channels=self.in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        # Group 2 time embedding\n",
    "        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n",
    "\n",
    "        # Group 3\n",
    "        self.normlize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.match_input = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1)\n",
    "        else:\n",
    "            self.match_input = nn.Identity()\n",
    "\n",
    "        if apply_attention:\n",
    "            self.attention = AttentionBlock(channels=self.out_channels)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # group 1\n",
    "        h = self.act_fn(self.normlize_1(x))\n",
    "        h = self.conv_1(h)\n",
    "\n",
    "        # group 2\n",
    "        # add in timestep embedding\n",
    "        h += self.dense_1(self.act_fn(t))[:, :, None, None]\n",
    "\n",
    "        # group 3\n",
    "        h = self.act_fn(self.normlize_2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv_2(h)\n",
    "\n",
    "        # Residual and attention\n",
    "        h = h + self.match_input(x)\n",
    "        h = self.attention(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        base_channels=128,\n",
    "        base_channels_multiples=(1, 2, 4, 8),\n",
    "        apply_attention=(False, False, True, False),\n",
    "        dropout_rate=0.1,\n",
    "        time_multiple=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        time_emb_dims_exp = base_channels * time_multiple\n",
    "        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels, time_emb_dims_exp=time_emb_dims_exp)\n",
    "\n",
    "        self.first = nn.Conv2d(in_channels=input_channels, out_channels=base_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        num_resolutions = len(base_channels_multiples)\n",
    "\n",
    "        # Encoder part of the UNet. Dimension reduction.\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        curr_channels = [base_channels]\n",
    "        in_channels = base_channels\n",
    "\n",
    "        for level in range(num_resolutions):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "\n",
    "            for _ in range(num_res_blocks):\n",
    "\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level],\n",
    "                )\n",
    "                self.encoder_blocks.append(block)\n",
    "\n",
    "                in_channels = out_channels\n",
    "                curr_channels.append(in_channels)\n",
    "\n",
    "            if level != (num_resolutions - 1):\n",
    "                self.encoder_blocks.append(DownSample(channels=in_channels))\n",
    "                curr_channels.append(in_channels)\n",
    "\n",
    "        # Bottleneck in between\n",
    "        self.bottleneck_blocks = nn.ModuleList(\n",
    "            (\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=True,\n",
    "                ),\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=False,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Decoder part of the UNet. Dimension restoration with skip-connections.\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "\n",
    "        for level in reversed(range(num_resolutions)):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                encoder_in_channels = curr_channels.pop()\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=encoder_in_channels + in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level],\n",
    "                )\n",
    "\n",
    "                in_channels = out_channels\n",
    "                self.decoder_blocks.append(block)\n",
    "\n",
    "            if level != 0:\n",
    "                self.decoder_blocks.append(UpSample(in_channels))\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=8, num_channels=in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        #print(\"Input Shape:\", x.shape)\n",
    "        \n",
    "        time_emb = self.time_embeddings(t)\n",
    "\n",
    "        h = self.first(x)\n",
    "        outs = [h]\n",
    "        #print(\"After First Convolution (Encoder Input):\", h.shape)\n",
    "        \n",
    "        for layer in self.encoder_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "            outs.append(h)\n",
    "            #print(f\"Encoder Block Output Shape:\", h.shape)\n",
    "        \n",
    "        for layer in self.bottleneck_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "            #print(f\"Bottleneck Block Output Shape:\", h.shape)\n",
    "            \n",
    "        for layer in self.decoder_blocks:\n",
    "            if isinstance(layer, ResnetBlock):\n",
    "                out = outs.pop()\n",
    "                h = torch.cat([h, out], dim=1)\n",
    "            h = layer(h, time_emb)\n",
    "            #print(f\"Decoder Block Output Shape:\", h.shape)\n",
    "            \n",
    "        h = self.final(h)\n",
    "        #print(\"Output Shape:\", h.shape)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "def get_default_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)\n",
    "\n",
    "def get(element: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get value at index position \"t\" in \"element\" and\n",
    "        reshape it to have the same dimension as a batch of images.\n",
    "    \"\"\"\n",
    "    ele = element.gather(-1, t)\n",
    "    return ele.reshape(-1, 1, 1, 1)\n",
    "\n",
    "def setup_log_directory(config):\n",
    "    '''Log and Model checkpoint directory Setup'''\n",
    "\n",
    "    if os.path.isdir(config.root_log_dir):\n",
    "        # Get all folders numbers in the root_log_dir\n",
    "        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(config.root_log_dir)]\n",
    "\n",
    "        # Find the latest version number present in the log_dir\n",
    "        last_version_number = max(folder_numbers)\n",
    "\n",
    "        # New version name\n",
    "        version_name = f\"version_{last_version_number + 1}\"\n",
    "\n",
    "    else:\n",
    "        version_name = config.log_dir\n",
    "\n",
    "    # Update the training config default directory\n",
    "    log_dir        = os.path.join(config.root_log_dir,        version_name)\n",
    "    checkpoint_dir = os.path.join(config.root_checkpoint_dir, version_name)\n",
    "\n",
    "    # Create new directory for saving new experiment version\n",
    "    os.makedirs(log_dir,        exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Logging at: {log_dir}\")\n",
    "    print(f\"Model Checkpoint at: {checkpoint_dir}\")\n",
    "\n",
    "    return log_dir, checkpoint_dir\n",
    "\n",
    "def frames2vid(images, save_path):\n",
    "\n",
    "    WIDTH = images[0].shape[1]\n",
    "    HEIGHT = images[0].shape[0]\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#     fourcc = 0\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(save_path, fourcc, 25, (WIDTH, HEIGHT))\n",
    "\n",
    "    # Appending the images to the video one by one\n",
    "    for image in images:\n",
    "        video.write(image)\n",
    "\n",
    "    # Deallocating memories taken for window creation\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    return\n",
    "\n",
    "def display_gif(gif_path):\n",
    "    b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
    "    display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BaseConfig:\n",
    "    DEVICE = get_default_device()\n",
    "    DATASET = \"Flowers\" #  \"MNIST\", \"Cifar-10\", \"Cifar-100\", \"Flowers\"\n",
    "\n",
    "    # For logging inferece images and saving checkpoints.\n",
    "    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Inference\")\n",
    "    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"checkpoints\")\n",
    "\n",
    "    # Current log and checkpoint directory.\n",
    "    log_dir = \"version_0\"\n",
    "    checkpoint_dir = \"version_0\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    TIMESTEPS = 1000 # Define number of diffusion timesteps\n",
    "    IMG_SHAPE = (1, 32, 32) if BaseConfig.DATASET == \"MNIST\" else (3, 128, 128)\n",
    "    NUM_EPOCHS = 800\n",
    "    BATCH_SIZE = 2\n",
    "    LR = 2e-5 # original = 2e - 5\n",
    "    NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset & Build Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(dataset_name='MNIST'):\n",
    "    transforms = TF.Compose(\n",
    "        [\n",
    "            TF.ToTensor(),\n",
    "            TF.Resize((128,128),\n",
    "                      interpolation=TF.InterpolationMode.BICUBIC,\n",
    "                      antialias=True),\n",
    "            TF.RandomHorizontalFlip(),\n",
    "            TF.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    if dataset_name.upper() == \"MNIST\":\n",
    "        dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transforms)\n",
    " \n",
    "    elif dataset_name == \"Flowers\":\n",
    "        #dataset = datasets.ImageFolder(root=\"/home/shashank/ShashankCode/Flowers_dataset\", transform=transforms)\n",
    "        dataset = datasets.ImageFolder(root=\"/home/shashank/All_code/Research/APTOS_DDPM/Image_seprated_class/Img_370\", transform=transforms)\n",
    "    return dataset\n",
    "\n",
    "def get_dataloader(dataset_name='MNIST',\n",
    "                   batch_size=32,\n",
    "                   pin_memory=False,\n",
    "                   shuffle=True,\n",
    "                   num_workers=0,\n",
    "                   device=\"cpu\"\n",
    "                  ):\n",
    "    dataset    = get_dataset(dataset_name=dataset_name)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size,\n",
    "                            pin_memory=pin_memory,\n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=shuffle\n",
    "                           )\n",
    "    device_dataloader = DeviceDataLoader(dataloader, device)\n",
    "    return device_dataloader\n",
    "\n",
    "def inverse_transform(tensors):\n",
    "    \"\"\"Convert tensors from [-1., 1.] to [0., 255.]\"\"\"\n",
    "    return ((tensors.clamp(-1, 1) + 1.0) / 2.0) * 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_dataloader(\n",
    "    dataset_name=BaseConfig.DATASET,\n",
    "    batch_size=128,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), facecolor='white')\n",
    "\n",
    "for b_image, _ in loader:\n",
    "    b_image = inverse_transform(b_image).cpu()\n",
    "    grid_img = make_grid(b_image / 255.0, nrow=6, padding=True, pad_value=1, normalize=True)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    print(b_image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_diffusion_timesteps=1000,\n",
    "        img_shape=(3, 64, 64),\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.num_diffusion_timesteps = num_diffusion_timesteps\n",
    "        self.img_shape = img_shape\n",
    "        self.device = device\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        # BETAs & ALPHAs required at different places in the Algorithm.\n",
    "        self.beta  = self.get_betas()\n",
    "        self.alpha = 1 - self.beta\n",
    "\n",
    "        self_sqrt_beta                       = torch.sqrt(self.beta)\n",
    "        self.alpha_cumulative                = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sqrt_alpha_cumulative           = torch.sqrt(self.alpha_cumulative)\n",
    "        self.one_by_sqrt_alpha               = 1. / torch.sqrt(self.alpha)\n",
    "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative)\n",
    "\n",
    "    def get_betas(self):\n",
    "        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n",
    "        scale = 1000 / self.num_diffusion_timesteps\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(\n",
    "            beta_start,\n",
    "            beta_end,\n",
    "            self.num_diffusion_timesteps,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "def forward_diffusion(sd: SimpleDiffusion, x0: torch.Tensor, timesteps: torch.Tensor):\n",
    "    eps = torch.randn_like(x0)  # Noise\n",
    "    mean    = get(sd.sqrt_alpha_cumulative, t=timesteps) * x0  # Image scaled\n",
    "    std_dev = get(sd.sqrt_one_minus_alpha_cumulative, t=timesteps) # Noise scaled\n",
    "    sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
    "\n",
    "    return sample, eps  # return ... , gt noise --> model predicts this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Forward Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SimpleDiffusion(num_diffusion_timesteps=TrainingConfig.TIMESTEPS, device=\"cpu\")\n",
    "\n",
    "loader = iter(  # converting dataloader into an iterator for now.\n",
    "    get_dataloader(\n",
    "        dataset_name=BaseConfig.DATASET,\n",
    "        batch_size=6,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0s, _ = next(loader)\n",
    "\n",
    "noisy_images = []\n",
    "specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n",
    "\n",
    "for timestep in specific_timesteps:\n",
    "    timestep = torch.as_tensor(timestep, dtype=torch.long)\n",
    "\n",
    "    xts, _ = forward_diffusion(sd, x0s, timestep)\n",
    "    xts = inverse_transform(xts) / 255.0\n",
    "    xts = make_grid(xts, nrow=1, padding=1)\n",
    "\n",
    "    noisy_images.append(xts)\n",
    "\n",
    "# Plot and see samples at different timesteps\n",
    "\n",
    "_, ax = plt.subplots(1, len(noisy_images), figsize=(10, 5), facecolor=\"white\")\n",
    "\n",
    "for i, (timestep, noisy_sample) in enumerate(zip(specific_timesteps, noisy_images)):\n",
    "    ax[i].imshow(noisy_sample.squeeze(0).permute(1, 2, 0))\n",
    "    ax[i].set_title(f\"t={timestep}\", fontsize=8)\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].grid(False)\n",
    "\n",
    "plt.suptitle(\"Forward Diffusion Process\", y=0.9)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    BASE_CH = 64  # 64, 128, 256, 256\n",
    "    BASE_CH_MULT = (1, 2, 4, 4) # 32, 16, 8, 8\n",
    "    APPLY_ATTENTION = (False, True, True, False)\n",
    "    DROPOUT_RATE = 0.1\n",
    "    TIME_EMB_MULT = 4 # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
    "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ")\n",
    "model.to(BaseConfig.DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig.LR)\n",
    "\n",
    "dataloader = get_dataloader(\n",
    "    dataset_name  = BaseConfig.DATASET,\n",
    "    batch_size    = TrainingConfig.BATCH_SIZE,\n",
    "    device        = BaseConfig.DEVICE,\n",
    "    pin_memory    = True,\n",
    "    num_workers   = TrainingConfig.NUM_WORKERS,\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
    "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
    "    device                  = BaseConfig.DEVICE,\n",
    ")\n",
    "\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs = TrainingConfig.NUM_EPOCHS + 1\n",
    "log_dir, checkpoint_dir = setup_log_directory(config=BaseConfig())\n",
    "\n",
    "generate_video = False\n",
    "ext = \".mp4\" if generate_video else \".png\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 1: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 1: Training\n",
    "\n",
    "def train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800,\n",
    "                   base_config=BaseConfig(), training_config=TrainingConfig()):\n",
    "\n",
    "    # print(\"model\", model)\n",
    "    # print(\"sd\", sd)\n",
    "    # print(\"loader\", loader)\n",
    "    # print(\"Optimizer\", optimizer)\n",
    "    # #print(\"scalar\",scalar)\n",
    "    # print(\"loss_fn\", loss_fn)\n",
    "    # print(\"Check params set in Base config and Training config\")\n",
    "    \n",
    "    loss_record = MeanMetric()\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n",
    "        tq.set_description(f\"Train :: Epoch: {epoch}/{training_config.NUM_EPOCHS}\")\n",
    "\n",
    "        for x0s, _ in loader:\n",
    "            tq.update(1)\n",
    "\n",
    "            ts = torch.randint(low=1, high=training_config.TIMESTEPS, size=(x0s.shape[0],), device=base_config.DEVICE)\n",
    "            xts, gt_noise = forward_diffusion(sd, x0s, ts)\n",
    "\n",
    "            with amp.autocast(dtype=torch.float16):\n",
    "                # print(\"------xts-----\",xts.shape)\n",
    "                # print(\"------ts-----\",ts)\n",
    "\n",
    "                pred_noise = model(xts, ts)\n",
    "                loss = loss_fn(gt_noise, pred_noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_value = loss.detach().item()\n",
    "            loss_record.update(loss_value)\n",
    "\n",
    "            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "        mean_loss = loss_record.compute().item()\n",
    "\n",
    "        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(f'{name}: {param.dtype}')\n",
    "\n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm 2: Sampling and saving the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm 2: Sampling and saving the image \n",
    "\n",
    "@torch.no_grad()\n",
    "def reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64),\n",
    "                      num_images=5, nrow=8, device=\"cpu\",save_indv_folder=False, **kwargs):\n",
    "\n",
    "    x = torch.randn((num_images, *img_shape), device=device)\n",
    "    model.eval()\n",
    "    \n",
    "    if kwargs.get(\"generate_video\", False):\n",
    "        outs = []\n",
    "\n",
    "    for time_step in tqdm(iterable=reversed(range(1, timesteps)),\n",
    "                          total=timesteps-1, dynamic_ncols=False,\n",
    "                          desc=\"Sampling :: \", position=0):\n",
    "\n",
    "        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n",
    "        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n",
    "\n",
    "        predicted_noise = model(x, ts)\n",
    "\n",
    "        beta_t                            = get(sd.beta, ts)\n",
    "        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n",
    "        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts)\n",
    "\n",
    "        x = (\n",
    "            one_by_sqrt_alpha_t\n",
    "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "            + torch.sqrt(beta_t) * z\n",
    "        )\n",
    "       \n",
    "        if kwargs.get(\"generate_video\", False):\n",
    "            x_inv = inverse_transform(x).type(torch.uint8)\n",
    "            grid = make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n",
    "            outs.append(ndarr)\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False): # Generate and save video of the entire reverse process.\n",
    "        frames2vid(outs, kwargs['save_path'])\n",
    "        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n",
    "        return None\n",
    "\n",
    "    else: # Display and save the image at the final timestep of the reverse process.\n",
    "        \n",
    "        x = inverse_transform(x).type(torch.uint8)\n",
    "        # Create a folder for individual images if it doesn't exist\n",
    "        if (save_indv_folder == True):\n",
    "            timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            individual_images_folder = f'generated_images/individual_images_{timestamp}'\n",
    "            os.makedirs(individual_images_folder, exist_ok=True)\n",
    "        else:\n",
    "            individual_images_folder = f'individual_images_training/individual_images_{timestamp}'\n",
    "            os.makedirs(individual_images_folder, exist_ok=True)\n",
    "\n",
    "        for i in range(x.shape[0]):\n",
    "            # Save each individual image\n",
    "            individual_image_path = os.path.join(individual_images_folder, f'image_{i+1}.png')\n",
    "            #individual_image = inverse_transform(x[i]).type(torch.uint8)\n",
    "            TF.functional.to_pil_image(x[i]).save(individual_image_path)\n",
    "\n",
    "        grid = make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "        pil_image = TF.functional.to_pil_image(grid)\n",
    "        pil_image.save(kwargs['save_path'], format=save_path[-3:].upper())\n",
    "        display(pil_image)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, total_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Algorithm 1: Training\n",
    "    train_one_epoch(model, sd, dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        save_path = os.path.join(log_dir, f\"{epoch}{ext}\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        # Profiling memory usage\n",
    "        current_memory = memory_allocated()\n",
    "        max_memory = torch.cuda.max_memory_allocated()\n",
    "        cached_memory = memory_cached()\n",
    "\n",
    "        # Algorithm 2: Sampling\n",
    "        reverse_diffusion(model, sd, timesteps=TrainingConfig.TIMESTEPS, num_images=16, generate_video=generate_video,\n",
    "            save_path=save_path, img_shape=TrainingConfig.IMG_SHAPE, device=BaseConfig.DEVICE,\n",
    "        )\n",
    "        # print(f\"Epoch {epoch + 1}:\")\n",
    "        # print(f\"Current memory allocated: {current_memory / 1024 / 1024:.2f} MB\")\n",
    "        # print(f\"Max memory allocated: {max_memory / 1024 / 1024:.2f} MB\")\n",
    "        # print(f\"Cached memory: {cached_memory / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "        # clear_output()\n",
    "        checkpoint_dict = {\n",
    "            \"opt\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"ckpt.tar\"))\n",
    "        del checkpoint_dict\n",
    "        # Reset max memory allocated for the next epoch\n",
    "        reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
    "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ")\n",
    "#checkpoint_dir = 'Logs_Checkpoints/checkpoints/version_11'\n",
    "#print(checkpoint_dict)\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt.tar\"), map_location='cpu')['model'])\n",
    "\n",
    "model.to(BaseConfig.DEVICE)\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
    "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
    "    device                  = BaseConfig.DEVICE,\n",
    ")\n",
    "\n",
    "log_dir = \"inference_results\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(75):\n",
    "    generate_video = False\n",
    "\n",
    "    ext = \".mp4\" if generate_video else \".png\"\n",
    "    filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "    save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "    print(TrainingConfig.IMG_SHAPE)\n",
    "    reverse_diffusion(\n",
    "        model,\n",
    "        sd,\n",
    "        num_images=16,\n",
    "        generate_video=generate_video,\n",
    "        save_path=save_path,\n",
    "        timesteps=1000,\n",
    "        img_shape=TrainingConfig.IMG_SHAPE,\n",
    "        device=BaseConfig.DEVICE,\n",
    "        nrow=8,\n",
    "        save_indv_folder=True,\n",
    "        \n",
    "    )\n",
    "\n",
    "    print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
