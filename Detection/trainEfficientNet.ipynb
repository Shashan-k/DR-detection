{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import os\n",
    "import config\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "# from efficientnet_pytorch import EfficientNet\n",
    "from dataset import DRDataset\n",
    "import datetime\n",
    "from torchvision.utils import save_image\n",
    "from earlystopper import EarlyStopper\n",
    "import csv \n",
    "from utilscopy import (\n",
    "    load_checkpoint,\n",
    "    save_checkpoint,\n",
    "    check_accuracy,\n",
    "    make_prediction,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMetric:\n",
    "    def __init__(self,actual_labels,predicted_labels):\n",
    "        ''' Initialization of variables '''\n",
    "        self.actual_labels = actual_labels\n",
    "        self.predicted_labels = predicted_labels\n",
    "    def single_value_conversion(self):\n",
    "        ''' This function is used for Converting model predicted values into single values\n",
    "           Ex: model_predicted_value: [0,1,0,0,0] and it converts as 1'''\n",
    "        predicted_labels = self.predicted_labels > 0.5\n",
    "        prediction_ordinal = np.empty(predicted_labels.shape, dtype = int)\n",
    "        prediction_ordinal[:,4] = predicted_labels[:,4]\n",
    "        for i in range(3, -1, -1): prediction_ordinal[:, i] = np.logical_or(predicted_labels[:,i], prediction_ordinal[:,i+1])\n",
    "        self.predicted_labels = prediction_ordinal.sum(axis = 1)-1\n",
    "        self.actual_labels = self.actual_labels.sum(axis = 1)-1\n",
    "    def confusionMatrix(self):\n",
    "        ''' This function is used for calculating confusion matrix between model predicted values and true values using sklearn implementation.'''\n",
    "        confusion_matrix_ = confusion_matrix(self.actual_labels, self.predicted_labels)\n",
    "        return confusion_matrix_\n",
    "    def precision(self, matrix):\n",
    "        ''' This function is used for calculating precision matrix between predicted values and true values using confusion matrix'''\n",
    "        precision_matrix =(((matrix.T)/(matrix.sum(axis=1))).T)\n",
    "        return precision_matrix\n",
    "    def recall(self, matrix):\n",
    "        ''' this function is used for calculating recall matrix between predicted values and true values using confusion matrix'''\n",
    "        recall_matrix =(matrix/matrix.sum(axis=0))\n",
    "        return recall_matrix\n",
    "    def subplot_(self, matrix, i, title):\n",
    "        ''' This function is used for subplots'''\n",
    "        plt.subplot(1,3,i)\n",
    "        labels = [1,2,3,4,5]\n",
    "        sns.heatmap(matrix, annot=True, cmap=sns.light_palette('green'),linewidths = 0.8,cbar = False, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Predicted Class Labels')\n",
    "        plt.ylabel('Actual Class Labels')\n",
    "    def plotting(self):\n",
    "        \"\"\"\n",
    "        This function is used for calculating number of misclassified points, confusion, recall and precision matrixes and plotting it using subplots.\n",
    "        \"\"\"\n",
    "        self.single_value_conversion()\n",
    "        confusion_matrix = self.confusionMatrix()\n",
    "        #print(\"Number of misclassified points: \",(len(self.actual_labels)-np.trace(confusion_matrix))/len(self.actual_labels)*100,\"\\n\")\n",
    "        precision_matrix = self.precision(confusion_matrix)\n",
    "        recall_matrix = self.recall(confusion_matrix)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        self.subplot_(confusion_matrix, 1, 'Confusion Matrix')\n",
    "        self.subplot_(precision_matrix, 2, 'Precision')\n",
    "        self.subplot_(recall_matrix, 3, 'Recall')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, optimizer, loss_fn, scaler, device):\n",
    "    losses = []\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data, targets, _) in enumerate(loop):\n",
    "        \n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            scores = model(data)\n",
    "            loss = loss_fn(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Loss average over epoch: {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_combinations = [\n",
    "{'batch_size': 64, 'learning_rate': 0.0002, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 2e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.005, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0005, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 5e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.003, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0003, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 3e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.001, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0001, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 1e-05, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.002, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.0002, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 2e-05, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.005, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.0005, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 5e-05, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.003, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.0003, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 3e-05, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.001, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 700},\n",
    "{'batch_size': 32, 'learning_rate': 1e-05, 'epochs': 700},\n",
    "]\n",
    "#131,64,3e-05,0,82.90636042402826,78.10476751030018,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision import models\n",
    "\n",
    "def training_main(hyperparams,early_stopper):\n",
    "    BATCH_SIZE = hyperparams['batch_size']\n",
    "    LEARNING_RATE = hyperparams['learning_rate']\n",
    "    NUM_EPOCHS = hyperparams['epochs']\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_train_accuracy = 0.0\n",
    "    best_train_precision = 0.0\n",
    "    best_train_recall = 0.0\n",
    "    best_train_f1 = 0.0\n",
    "        \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_folder = f\"Model_weights/DenseNet201_RealSynth_{current_time}_{BATCH_SIZE}_{LEARNING_RATE}\"\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "\n",
    "    train_ds = DRDataset(\n",
    "        images_folder=\"/home/shashank/All_Code/Research/Dataset/APTOS_RS_192/Real_Synthetic/RS_train_split\",\n",
    "        path_to_csv=\"/home/shashank/All_Code/Research/Dataset/APTOS_RS_192/Real_Synthetic/RS_train_split.csv\",\n",
    "        transform=config.val_transforms,\n",
    "    )\n",
    "    val_ds = DRDataset(\n",
    "        images_folder=\"/home/shashank/All_Code/Research/Dataset/APTOS_RS_192/Real_Synthetic/RS_val_split\",\n",
    "        path_to_csv=\"/home/shashank/All_Code/Research/Dataset/APTOS_RS_192/Real_Synthetic/RS_val_split.csv\",\n",
    "        transform=config.val_transforms,\n",
    "    )\n",
    "    # test_ds = DRDataset(\n",
    "    #     images_folder=\"/home/shashank/All_code/test/images_resized_192\",\n",
    "    #     path_to_csv=\"/home/shashank/All_code/Research/APTOS_dataset/aptos2019_blindness_detection/test.csv\",\n",
    "    #     transform=config.val_transforms,\n",
    "    #     train=False,\n",
    "    # )\n",
    "    # test_loader = DataLoader(\n",
    "    #     test_ds, batch_size=BATCH_SIZE, num_workers=6, shuffle=False\n",
    "    # )\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "    )\n",
    "   \n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS, \n",
    "        pin_memory=config.PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    ########### MODEL ################\n",
    "    \n",
    "    model = models.densenet201(pretrained=True)\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        \n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(num_features, 5)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #model = models.resnet50(pretrained=True)\n",
    "    #model.fc = nn.Linear(model.fc.in_features, 5)\n",
    "    \n",
    "    # model = models.alexnet(pretrained=True)\n",
    "    # model.classifier[6] = nn.Linear(model.classifier[6].in_features, 5)\n",
    "    \n",
    "    model = model.to(config.DEVICE)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    if config.LOAD_MODEL and config.CHECKPOINT_FILE in os.listdir():\n",
    "        load_checkpoint(torch.load(config.CHECKPOINT_FILE), model, optimizer,LEARNING_RATE)\n",
    "        print(\"Model Loaded !!!!!!!!\")\n",
    "\n",
    "    # Run after training is done and you've achieved good result\n",
    "    # on validation set, then run train_blend.py file to use information\n",
    "    # about both eyes concatenated\n",
    "    #get_csv_for_blend(val_loader, model, \"../train/val_blend.csv\")\n",
    "    #get_csv_for_blend(train_loader, model, \"../train/train_blend.csv\")\n",
    "    #get_csv_for_blend(test_loader, model, \"../train/test_blend.csv\")\n",
    "    #make_prediction(model, test_loader, \"submission.csv\")\n",
    "    #import sys\n",
    "    #sys.exit()\n",
    "    #make_prediction(model, test_loader)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"Epoch - \", epoch)\n",
    "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, config.DEVICE)\n",
    "\n",
    "        # get on validation\n",
    "        preds, labels,val_accuracy, val_precision, val_recall, val_f1 = check_accuracy(val_loader, model, config.DEVICE)\n",
    "        print(\"Validation accuracy : \",val_accuracy)\n",
    "        quad_score_val = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "        print(f\"QuadraticWeightedKappa (Validation): \", quad_score_val)\n",
    "        \n",
    "        # get on train\n",
    "        predsontrain, labelsontrain, train_accuracy,train_precision,train_recall, train_f1 = check_accuracy(train_loader, model, config.DEVICE)\n",
    "        print(\"Training accuracy : \",train_accuracy)\n",
    "        quad_score_train = cohen_kappa_score(labelsontrain, predsontrain, weights='quadratic')\n",
    "        print(f\"QuadraticWeightedKappa (Train): \", quad_score_train)\n",
    "\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_train_accuracy = train_accuracy\n",
    "            best_accuracy = val_accuracy\n",
    "            epoch_checkpoint = epoch+1\n",
    "            best_train_precision = train_precision\n",
    "            best_precision = val_precision\n",
    "            best_train_recall = train_recall\n",
    "            best_recall = val_recall\n",
    "            best_train_f1 = train_f1\n",
    "            best_f1 = val_f1\n",
    "            checkpoint = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                }\n",
    "            save_checkpoint(checkpoint, filename=os.path.join(output_folder+\"/b3_.pth.tar\"))\n",
    "        \n",
    "        if early_stopper.early_stop(val_accuracy):\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "        \n",
    "    return epoch, best_accuracy, best_train_accuracy, best_train_precision, best_train_recall, best_train_f1, best_precision, best_recall, best_f1, quad_score_val   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter_combinations >>>>>>>>>>>>>>  {'batch_size': 64, 'learning_rate': 0.0002, 'epochs': 700}\n",
      "Epoch -  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/107 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(\"/home/shashank/All_Code/Research/APTOS_Detection/Model_weights\", f\"{current_time}_DenseNet201_REALSynth_log.csv\")\n",
    "with open(log_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['epoch','batch_size','Learning_rate', 'training_loss', 'training_accuracy',\n",
    "                  'train_precision','train_recall','train_f1','val_accuracy', 'val_loss','val_precision','val_recall','val_f1','Qudratic_kappa_score_val']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    torch.cuda.empty_cache()\n",
    "    # Instantiate EarlyStopper with desired patience and min_delta\n",
    "    early_stopper = EarlyStopper(patience=40, min_delta=0.001)\n",
    "    print(\"hyperparameter_combinations >>>>>>>>>>>>>> \",hyperparams)\n",
    "    epoch , best_accuracy,best_train_accuracy,best_train_precision,best_train_recall,best_train_f1, best_precision, best_recall, best_f1, quad_score_val = training_main(hyperparams,early_stopper)\n",
    "    \n",
    "\n",
    "    with open(log_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({\n",
    "            'epoch': epoch,\n",
    "            'batch_size': hyperparams['batch_size'],\n",
    "            'Learning_rate': hyperparams['learning_rate'],\n",
    "            'training_loss': \"0\",\n",
    "            'training_accuracy': best_train_accuracy,\n",
    "            'train_precision' : best_train_precision,\n",
    "            'train_recall' : best_train_recall,\n",
    "            'train_f1' : best_train_f1,\n",
    "            \n",
    "            'val_accuracy': best_accuracy,\n",
    "            'val_loss' : \"0\",\n",
    "            'val_precision' : best_precision,\n",
    "            'val_recall' : best_recall,\n",
    "            'val_f1' : best_f1,\n",
    "            'Qudratic_kappa_score_val': quad_score_val,\n",
    "            \n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
