{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shashank/All_code/Research/APTOS_Detection\n"
     ]
    }
   ],
   "source": [
    "cd APTOS_Detection/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import os\n",
    "import config\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from dataset import DRDataset\n",
    "import datetime\n",
    "from torchvision.utils import save_image\n",
    "from earlystopper import EarlyStopper\n",
    "import csv \n",
    "from utils import (\n",
    "    load_checkpoint,\n",
    "    save_checkpoint,\n",
    "    check_accuracy,\n",
    "    make_prediction,\n",
    "    get_csv_for_blend,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "\n",
    "\n",
    "# Set a specific seed value (you can use any integer)\n",
    "seed = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(loader, model, optimizer, loss_fn, scaler, device):\n",
    "    losses = []\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data, targets, _) in enumerate(loop):\n",
    "        # save examples and make sure they look ok with the data augmentation,\n",
    "        # tip is to first set mean=[0,0,0], std=[1,1,1] so they look \"normal\"\n",
    "        #save_image(data, f\"hi_{batch_idx}.png\")\n",
    "\n",
    "        data = data.to(device=device)\n",
    "        #print(data.shape)\n",
    "        #print(\"Targets\",targets)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            scores = model(data)\n",
    "            loss = loss_fn(scores, targets.unsqueeze(1).float())\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Loss average over epoch: {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_combinations = [\n",
    "{'batch_size': 64, 'learning_rate': 3e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0002, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 2e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.005, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0005, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 5e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.003, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0003, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 3e-05, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.001, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 0.0001, 'epochs': 700},\n",
    "{'batch_size': 64, 'learning_rate': 1e-05, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.002, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.0002, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 2e-05, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.005, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.0005, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 5e-05, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.003, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.0003, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 3e-05, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.001, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 0.0001, 'epochs': 700},\n",
    "# {'batch_size': 32, 'learning_rate': 1e-05, 'epochs': 700},\n",
    "]\n",
    "#131,64,3e-05,0,82.90636042402826,78.10476751030018,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_main(hyperparams,early_stopper):\n",
    "    BATCH_SIZE = hyperparams['batch_size']\n",
    "    LEARNING_RATE = hyperparams['learning_rate']\n",
    "    NUM_EPOCHS = hyperparams['epochs']\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "    best_precision = 0.0\n",
    "    best_recall = 0.0\n",
    "    best_train_accuracy = 0.0\n",
    "    best_f1 = 0.0\n",
    "    best_train_precision = 0\n",
    "    best_train_recall = 0\n",
    "        \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_folder = f\"output_results/training_{current_time}_{BATCH_SIZE}_{LEARNING_RATE}\"\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "\n",
    "    \n",
    "    train_ds = DRDataset(\n",
    "        images_folder=\"/home/shashank/All_code/Research/APTOS_dataset/aptos2019_blindness_detection/Real_Synthetic/RS_train_split\",\n",
    "        path_to_csv=\"/home/shashank/All_code/Research/APTOS_dataset/aptos2019_blindness_detection/Real_Synthetic/RS_train_split.csv\",\n",
    "        transform=config.val_transforms,\n",
    "    )\n",
    "    val_ds = DRDataset(\n",
    "        images_folder=\"/home/shashank/All_code/Research/APTOS_dataset/aptos2019_blindness_detection/Real_Synthetic/RS_val_split\",\n",
    "        path_to_csv=\"/home/shashank/All_code/Research/APTOS_dataset/aptos2019_blindness_detection/Real_Synthetic/RS_val_split.csv\",\n",
    "        transform=config.val_transforms,\n",
    "    )\n",
    "    test_ds = DRDataset(\n",
    "        images_folder=\"/home/shashank/All_code/test/images_resized_192\",\n",
    "        path_to_csv=\"/home/shashank/All_code/Research/APTOS_dataset/aptos2019_blindness_detection/test.csv\",\n",
    "        transform=config.val_transforms,\n",
    "        train=False,\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_ds, batch_size=BATCH_SIZE, num_workers=6, shuffle=False\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=config.PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "    )\n",
    "   \n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS, \n",
    "        pin_memory=config.PIN_MEMORY,\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b7\")\n",
    "    #model = EfficientNet.from_name(\"efficientnet-b6\")\n",
    "    model._fc = nn.Linear(2560, 1)\n",
    "    model = model.to(config.DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    print(\"OS Path \",os.listdir())    \n",
    "    if config.LOAD_MODEL and config.CHECKPOINT_FILE in os.listdir():\n",
    "        load_checkpoint(torch.load(config.CHECKPOINT_FILE), model, optimizer,LEARNING_RATE)\n",
    "        print(\"Model Loaded !!!!!!!!\")\n",
    "\n",
    "    #model._fc = nn.Linear(1536,1)\n",
    "        \n",
    "    # Run after training is done and you've achieved good result\n",
    "    # on validation set, then run train_blend.py file to use information\n",
    "    # about both eyes concatenated\n",
    "    #get_csv_for_blend(val_loader, model, \"../train/val_blend.csv\")\n",
    "    #get_csv_for_blend(train_loader, model, \"../train/train_blend.csv\")\n",
    "    #get_csv_for_blend(test_loader, model, \"../train/test_blend.csv\")\n",
    "    #make_prediction(model, test_loader, \"submission.csv\")\n",
    "    #import sys\n",
    "    #sys.exit()\n",
    "    #make_prediction(model, test_loader)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"Epoch - \", epoch)\n",
    "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, config.DEVICE)\n",
    "\n",
    "        # get on validation\n",
    "        preds, labels,val_accuracy, val_precision, val_recall, val_f1 = check_accuracy(val_loader, model, config.DEVICE)\n",
    "        quad_score_val = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "        # print(f\"QuadraticWeightedKappa (Validation): {cohen_kappa_score(labels, preds, weights='quadratic')}\")\n",
    "        print(f\"QuadraticWeightedKappa (Validation): \", quad_score_val)\n",
    "        \n",
    "        # get on train\n",
    "        preds, labels,train_accuracy,train_precision,train_recall, train_f1 = check_accuracy(train_loader, model, config.DEVICE)\n",
    "        quad_score_train = cohen_kappa_score(labels, preds, weights='quadratic')\n",
    "        print(f\"QuadraticWeightedKappa (Training): {cohen_kappa_score(labels, preds, weights='quadratic')}\")\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_train_accuracy = train_accuracy\n",
    "            best_accuracy = val_accuracy\n",
    "            epoch_checkpoint = epoch+1\n",
    "            \n",
    "        if val_precision > best_precision:\n",
    "            #best_train_precision = train_precision\n",
    "            best_precision = val_precision\n",
    "            \n",
    "        if val_recall > best_recall:\n",
    "            #best_train_recall = train_recall\n",
    "            best_recall = val_recall\n",
    "            \n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            print()\n",
    "\n",
    "           \n",
    "        #if config.SAVE_MODEL and epoch % 10 == 0 :\n",
    "            if config.SAVE_MODEL:\n",
    "                checkpoint = {\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                }\n",
    "            #save_checkpoint(checkpoint, filename=f\"/home/shashank/All_code/Research/APTOS_Detection/weights/b3_{epoch}.pth.tar\")\n",
    "            save_checkpoint(checkpoint, filename=os.path.join(output_folder+\"/b3_.pth.tar\"))\n",
    "\n",
    "        \n",
    "        if early_stopper.early_stop(val_accuracy):\n",
    "            \n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    return epoch, best_accuracy, best_train_accuracy, best_precision, best_recall, best_f1, quad_score_val        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter_combinations >>>>>>>>>>>>>>  {'batch_size': 64, 'learning_rate': 3e-05, 'epochs': 1}\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "OS Path  ['submission.csv', 'output_results', 'weights', 'val1csv.csv', '__pycache__', 'train_blend.py', 'fidelity', 'config.py', 'preprocess_images.py', 'earlystopper.py', 'utils.py', 'train1', 'train.py', 'train.ipynb', 'val1', 'dataset.py', 'train1csv.csv']\n",
      "Epoch -  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:27<00:00,  3.93it/s, loss=1.44] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss average over epoch: 4.6927641858564355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:04<00:00,  6.09it/s]\n",
      "/home/shashank/anaconda3/envs/Cuda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 521 / 1699 with accuracy 30.67\n",
      "Shapes - all_preds: (1699,) all_labels: (1699,)\n",
      "DataType - all_preds: int64 all_labels: int64\n",
      "Unique labels: [0 1 2 3 4]\n",
      "Accuracy: 30.67\n",
      "Precision: 0.35\n",
      "Recall: 0.31\n",
      "F1 Score: 0.20\n",
      "QuadraticWeightedKappa (Validation):  0.1951168651204248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:16<00:00,  6.54it/s]\n",
      "/home/shashank/anaconda3/envs/Cuda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2093 / 6792 with accuracy 30.82\n",
      "Shapes - all_preds: (6792,) all_labels: (6792,)\n",
      "DataType - all_preds: int64 all_labels: int64\n",
      "Unique labels: [0 1 2 3 4]\n",
      "Accuracy: 30.82\n",
      "Precision: 0.16\n",
      "Recall: 0.31\n",
      "F1 Score: 0.20\n",
      "QuadraticWeightedKappa (Training): 0.2007593907074502\n",
      "\n",
      "=> Saving checkpoint\n",
      "hyperparameter_combinations >>>>>>>>>>>>>>  {'batch_size': 64, 'learning_rate': 0.0002, 'epochs': 1}\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "OS Path  ['submission.csv', 'output_results', 'weights', 'val1csv.csv', '__pycache__', 'train_blend.py', 'fidelity', 'config.py', 'preprocess_images.py', 'earlystopper.py', 'utils.py', 'train1', 'train.py', 'train.ipynb', 'val1', 'dataset.py', 'train1csv.csv']\n",
      "Epoch -  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:26<00:00,  4.00it/s, loss=1.24] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss average over epoch: 2.6760690687137227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:04<00:00,  6.17it/s]\n",
      "/home/shashank/anaconda3/envs/Cuda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 652 / 1699 with accuracy 38.38\n",
      "Shapes - all_preds: (1699,) all_labels: (1699,)\n",
      "DataType - all_preds: int64 all_labels: int64\n",
      "Unique labels: [0 1 2 3 4]\n",
      "Accuracy: 38.38\n",
      "Precision: 0.25\n",
      "Recall: 0.38\n",
      "F1 Score: 0.30\n",
      "QuadraticWeightedKappa (Validation):  0.6116447378584242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [00:16<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2520 / 6792 with accuracy 37.10\n",
      "Shapes - all_preds: (6792,) all_labels: (6792,)\n",
      "DataType - all_preds: int64 all_labels: int64\n",
      "Unique labels: [0 1 2 3 4]\n",
      "Accuracy: 37.10\n",
      "Precision: 0.45\n",
      "Recall: 0.37\n",
      "F1 Score: 0.28\n",
      "QuadraticWeightedKappa (Training): 0.6299566799889269\n",
      "\n",
      "=> Saving checkpoint\n",
      "hyperparameter_combinations >>>>>>>>>>>>>>  {'batch_size': 64, 'learning_rate': 2e-05, 'epochs': 700}\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "OS Path  ['submission.csv', 'output_results', 'weights', 'val1csv.csv', '__pycache__', 'train_blend.py', 'fidelity', 'config.py', 'preprocess_images.py', 'earlystopper.py', 'utils.py', 'train1', 'train.py', 'train.ipynb', 'val1', 'dataset.py', 'train1csv.csv']\n",
      "Epoch -  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 21/107 [00:05<00:23,  3.71it/s, loss=8.7] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m early_stopper \u001b[38;5;241m=\u001b[39m EarlyStopper(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhyperparameter_combinations >>>>>>>>>>>>>> \u001b[39m\u001b[38;5;124m\"\u001b[39m,hyperparams)\n\u001b[0;32m---> 13\u001b[0m epoch , best_accuracy,best_train_accuracy, best_precision, best_recall, best_f1,quad_score_val \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43mearly_stopper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m     17\u001b[0m     writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(csvfile, fieldnames\u001b[38;5;241m=\u001b[39mfieldnames)\n",
      "Cell \u001b[0;32mIn[6], line 84\u001b[0m, in \u001b[0;36mtraining_main\u001b[0;34m(hyperparams, early_stopper)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch - \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch)\n\u001b[0;32m---> 84\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# get on validation\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     preds, labels,val_accuracy, val_precision, val_recall, val_f1 \u001b[38;5;241m=\u001b[39m check_accuracy(val_loader, model, config\u001b[38;5;241m.\u001b[39mDEVICE)\n",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     23\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     26\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/Cuda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    414\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 416\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/Cuda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/anaconda3/envs/Cuda/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    315\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = os.path.join(\"/home/shashank/All_code/Research/APTOS_Detection/output_results\", f\"{current_time}_ENb7_log.csv\")\n",
    "with open(log_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['epoch','batch_size','Learning_rate', 'training_loss', 'training_accuracy',  'val_accuracy', 'val_loss','val_precision','val_recall','val_f1','Qudratic_kappa_score_val']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "for hyperparams in hyperparameter_combinations:\n",
    "    torch.cuda.empty_cache()\n",
    "    # Instantiate EarlyStopper with desired patience and min_delta\n",
    "    early_stopper = EarlyStopper(patience=20, min_delta=0.001)\n",
    "    print(\"hyperparameter_combinations >>>>>>>>>>>>>> \",hyperparams)\n",
    "    epoch , best_accuracy,best_train_accuracy, best_precision, best_recall, best_f1,quad_score_val = training_main(hyperparams,early_stopper)\n",
    "    \n",
    "\n",
    "    with open(log_file, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({\n",
    "            'epoch': epoch,\n",
    "            'batch_size': hyperparams['batch_size'],\n",
    "            'Learning_rate': hyperparams['learning_rate'],\n",
    "            'training_loss': \"0\",\n",
    "            'training_accuracy': best_train_accuracy,\n",
    "            'val_accuracy': best_accuracy,\n",
    "            'val_loss' : \"0\",\n",
    "            'val_precision' : best_precision,\n",
    "            'val_recall' : best_recall,\n",
    "            'val_f1' : best_f1,\n",
    "            'Qudratic_kappa_score_val': quad_score_val,\n",
    "            \n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
